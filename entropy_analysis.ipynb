{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904b1d5-6fb9-4126-a68a-f10c34e603e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check GPU and CUDA\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a73928-8306-469c-af87-44678efa72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current working directory\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56f5b3-f862-4c4c-8e8a-16ac9ad39ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install necessary packages\n",
    "%pip install scikit-optimize\n",
    "%pip install opencv-python-headless\n",
    "%pip install seaborn\n",
    "%pip install umap-learn\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5f733-ef78-406b-ae63-b02f936c6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import other libraries\n",
    "# DO \"pip install [package-name]\" for libraries suiting your requirements\n",
    "import zipfile\n",
    "import os\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore',category=FutureWarning) \n",
    "import struct\n",
    "import zlib\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications import VGG16, DenseNet121, ResNet50, MobileNet, EfficientNetB0, InceptionV3\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dropout, GlobalAveragePooling2D, Dense,Conv2D, SeparableConv2D\n",
    "from sklearn.metrics import roc_curve, auc,  precision_recall_curve, average_precision_score, matthews_corrcoef\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score, precision_score, recall_score, classification_report, log_loss, confusion_matrix, accuracy_score \n",
    "from sklearn.utils import class_weight\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b1dd1-3117-4eef-ad06-9ebe720e8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check Tensorflow Version and GPU availability \n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "#print number of GPUs available to use\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.keras.backend.clear_session(\n",
    "    free_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd8a88-dd96-43a6-9a1b-0b74ad94a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove unwanted files and directories like ipython checkpoints and thumbs.db files and keep only the valid image files\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image  \n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "# Define the paths for instance and class data directories\n",
    "train_data_dir = \"path/to/train\"\n",
    "val_data_dir = \"path/to/val\"\n",
    "test_data_dir = \"path/to/test\"\n",
    "\n",
    "\n",
    "# List of unwanted directories and files to look for\n",
    "unwanted_directories = ['.ipynb_checkpoints']\n",
    "unwanted_files = ['thumbs.db', 'Thumbs.db', '.DS_Store', 'DS_Store']\n",
    "\n",
    "def is_valid_image(file_path):\n",
    "    \"\"\"\n",
    "    Function to check if a file is a valid image.\n",
    "    Returns True if the file is a valid image, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(file_path)\n",
    "        img.verify()  # Verify that it is, in fact, an image\n",
    "        return True\n",
    "    except (IOError, SyntaxError, UnidentifiedImageError):\n",
    "        return False\n",
    "\n",
    "#clean directories\n",
    "def clean_directory(directory):\n",
    "    \"\"\"\n",
    "    Function to check for unwanted directories, non-image files, and invalid image files\n",
    "    in the given directory and delete them.\n",
    "    \"\"\"\n",
    "    # Track the removed files and directories\n",
    "    removed_items = []\n",
    "\n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):  # Traverse bottom-up\n",
    "        # Remove unwanted files and invalid images\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "\n",
    "            if file_name in unwanted_files:\n",
    "                print(f\"Removing unwanted file: {file_path}\")\n",
    "                os.remove(file_path)  # Remove the unwanted file\n",
    "                removed_items.append(file_path)\n",
    "            elif not is_valid_image(file_path):\n",
    "                print(f\"Removing invalid image: {file_path}\")\n",
    "                os.remove(file_path)  # Remove the invalid image file\n",
    "                removed_items.append(file_path)\n",
    "\n",
    "        # Remove unwanted directories (now using shutil.rmtree to remove non-empty directories)\n",
    "        for dir_name in dirs:\n",
    "            if dir_name in unwanted_directories:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                print(f\"Removing directory: {dir_path}\")\n",
    "                shutil.rmtree(dir_path)  # Remove the directory and its contents\n",
    "                removed_items.append(dir_path)\n",
    "\n",
    "    return removed_items\n",
    "\n",
    "# Clean the directories\n",
    "print(\"Cleaning train_data_dir...\")\n",
    "removed_train_items = clean_directory(train_data_dir)\n",
    "\n",
    "print(\"Cleaning val_data_dir...\")\n",
    "removed_val_items = clean_directory(val_data_dir)\n",
    "\n",
    "print(\"Cleaning test_data_dir...\")\n",
    "removed_test_items = clean_directory(test_data_dir)\n",
    "\n",
    "# Output summary of removed items\n",
    "print(f\"\\nRemoved {len(removed_train_items)} items from train_data_dir.\")\n",
    "print(f\"\\nRemoved {len(removed_val_items)} items from val_data_dir.\")\n",
    "print(f\"\\nRemoved {len(removed_test_items)} items from test_data_dir.\")\n",
    "\n",
    "# If no items were removed\n",
    "if not removed_train_items and not removed_val_items and not removed_test_items:\n",
    "    print(\"No unwanted files, directories, or invalid images were found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b1d74-0a9b-451d-b1df-8a0413df37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of files in each sub-folder after cleaning\n",
    "\n",
    "def count_files(directory):\n",
    "    normal_path = os.path.join(directory, \"normal\") #class-1\n",
    "    viral_path = os.path.join(directory, \"viral\") #class -2\n",
    "    \n",
    "    normal_count = len([f for f in os.listdir(normal_path) if os.path.isfile(os.path.join(normal_path, f))])\n",
    "    viral_count = len([f for f in os.listdir(viral_path) if os.path.isfile(os.path.join(viral_path, f))])    \n",
    "    return normal_count, viral_count\n",
    "\n",
    "base_dir = \"path/to\"\n",
    "folders = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for folder in folders:\n",
    "    directory = os.path.join(base_dir, folder)\n",
    "    # Unpack all three values.\n",
    "    normal_count, viral_count = count_files(directory)\n",
    "    \n",
    "    print(f\"Number of files in the {folder} directory:\")\n",
    "    print(f\"  normal: {normal_count}\")\n",
    "    print(f\"  viral: {viral_count}\")\n",
    "    total = normal_count + viral_count\n",
    "    print(f\"  Total: {total}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49b64b-52a0-40c6-979e-f935c8d892be",
   "metadata": {},
   "source": [
    "Custom function to plot confusion matrix and print other metrics including confidence intervals for the MCC metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a6733-1306-4c50-9f2c-6a425b4b4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "def comp_metrix_binary(y_true, y_pred, beta=1, confidence=0.95, conf_vis=\"both\",\n",
    "                         annot_fontsize=12, tick_fontsize=12, title_fontsize=14,class_names=[\"Normal\", \"Viral\"]):\n",
    "    # Convert one-hot to integer labels if needed.\n",
    "    if np.ndim(y_true) > 1 and y_true.shape[1] > 1:\n",
    "        y_true_int = np.argmax(y_true, axis=1)\n",
    "    else:\n",
    "        y_true_int = np.array(y_true)\n",
    "    if np.ndim(y_pred) > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred_int = np.argmax(y_pred, axis=1)\n",
    "    else:\n",
    "        y_pred_int = np.array(y_pred)\n",
    "    \n",
    "    # Compute default scikit learn based confusion matrix (rows=Actual, cols=Predicted)\n",
    "    cm_default = confusion_matrix(y_true_int, y_pred_int, labels=[0, 1])\n",
    "    # Binary: [[TN, FP],\n",
    "    #          [FN, TP]]\n",
    "    TN = cm_default[0, 0]\n",
    "    FP = cm_default[0, 1]\n",
    "    FN = cm_default[1, 0]\n",
    "    TP = cm_default[1, 1]\n",
    "    \n",
    "    # Clinician's view: transpose of default\n",
    "    cm_clinician = cm_default.T\n",
    "    \n",
    "    # Visualization based on flag\n",
    "    if conf_vis in [\"both\", \"default\", \"clinician\"]:\n",
    "        if conf_vis == \"both\":\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5),dpi=300)\n",
    "            # Default view heatmap\n",
    "            sns.heatmap(cm_default, annot=True, fmt='d', cmap='cividis', ax=axes[0],\n",
    "                        xticklabels=class_names, yticklabels=class_names,\n",
    "                        annot_kws={\"size\": annot_fontsize})\n",
    "            axes[0].set_title(\"Default (Rows=Actual, Cols=Predicted)\", fontsize=title_fontsize)\n",
    "            axes[0].set_xlabel(\"Predicted Label\", fontsize=tick_fontsize)\n",
    "            axes[0].set_ylabel(\"Actual Label\", fontsize=tick_fontsize)\n",
    "            axes[0].tick_params(labelsize=tick_fontsize)\n",
    "            # Clinician's view heatmap\n",
    "            sns.heatmap(cm_clinician, annot=True, fmt='d', cmap='Greens', ax=axes[1], \n",
    "                        xticklabels=class_names, yticklabels=class_names,\n",
    "                        annot_kws={\"size\": annot_fontsize})\n",
    "            axes[1].set_title(\"Clinician's View (Rows=Predicted, Cols=Actual)\", fontsize=title_fontsize)\n",
    "            axes[1].set_xlabel(\"Actual Label\", fontsize=tick_fontsize)\n",
    "            axes[1].set_ylabel(\"Predicted Label\", fontsize=tick_fontsize)\n",
    "            axes[1].tick_params(labelsize=tick_fontsize)\n",
    "        else:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 5),dpi=300)\n",
    "            if conf_vis == \"default\":\n",
    "                sns.heatmap(cm_default, annot=True, fmt='d', cmap='cividis', ax=ax,\n",
    "                            xticklabels=class_names, yticklabels=class_names,\n",
    "                            annot_kws={\"size\": annot_fontsize})\n",
    "                ax.set_title(\"Default (Rows=Actual, Cols=Predicted)\", fontsize=title_fontsize)\n",
    "                ax.set_xlabel(\"Predicted Label\", fontsize=tick_fontsize)\n",
    "                ax.set_ylabel(\"Actual Label\", fontsize=tick_fontsize)\n",
    "            else:  # clinician view\n",
    "                sns.heatmap(cm_clinician, annot=True, fmt='d', cmap='Greens', ax=ax, \n",
    "                            xticklabels=class_names, yticklabels=class_names,\n",
    "                            annot_kws={\"size\": annot_fontsize})\n",
    "                ax.set_title(\"Clinician's View (Rows=Predicted, Cols=Actual)\", fontsize=title_fontsize)\n",
    "                ax.set_xlabel(\"Actual Label\", fontsize=tick_fontsize)\n",
    "                ax.set_ylabel(\"Predicted Label\", fontsize=tick_fontsize)\n",
    "            ax.tick_params(labelsize=tick_fontsize)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"confusion_matrix.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    Population = TN + FP + FN + TP\n",
    "    Accuracy = (TP + TN) / Population if Population != 0 else float('nan')\n",
    "    Recall = TP / (TP + FN) if (TP + FN) != 0 else float('nan')  #Recall/Sensitivity\n",
    "    Precision = TP / (TP + FP) if (TP + FP) != 0 else float('nan')  # PPV\n",
    "    Specificity = TN / (TN + FP) if (TN + FP) != 0 else float('nan')\n",
    "    F1 = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) != 0 else float('nan')\n",
    "    bal_acc = (Recall + Specificity) / 2\n",
    "    J = Recall + Specificity - 1\n",
    "    MCC_num = (TP * TN) - (FP * FN)\n",
    "    MCC_den = math.sqrt((TP+FP) * (TP+FN) * (TN+FP) * (TN+FN))\n",
    "    MCC = MCC_num / MCC_den if MCC_den != 0 else float('nan')\n",
    "    Kappa_den = (TP*FN + TP*FP + 2*TP*TN + FN**2 + FN*TN + FP**2 + FP*TN)\n",
    "    Kappa = 2 * (TP * TN - FN * FP) / Kappa_den if Kappa_den != 0 else float('nan')\n",
    "    \n",
    "    # Wilson binomial confidence interval for Accuracy:\n",
    "    z = 1.96\n",
    "    n = Population\n",
    "    p = MCC\n",
    "    lower_bound = (p + (z*z)/(2*n) - z * math.sqrt((p*(1-p) + (z*z)/(4*n))/n))/(1 + (z*z)/n) if n > 0 else float('nan')\n",
    "    upper_bound = (p + (z*z)/(2*n) + z * math.sqrt((p*(1-p) + (z*z)/(4*n))/n))/(1 + (z*z)/n) if n > 0 else float('nan')\n",
    "    \n",
    "    FBeta = (1+beta**2) * (Precision * Recall) / ((beta**2 * Precision) + Recall) if ((beta**2 * Precision) + Recall) != 0 else float('nan')\n",
    "    NPV = TN / (TN + FN) if (TN + FN) != 0 else float('nan')\n",
    "    \n",
    "    mat_met = pd.DataFrame({\n",
    "       'Metric': ['TP', 'TN', 'FP', 'FN', \n",
    "                  'Accuracy', 'Recall/Sensitivity', 'Balanced Accuracy', 'Youden', 'Specificity', 'Precision/PPV', 'NPV', 'F1', 'FBeta', 'MCC', 'Kappa',\n",
    "                  'Wilson_LowerBound', 'Wilson_UpperBound'],\n",
    "       'Value': [TP, TN, FP, FN, \n",
    "                 round(Accuracy,4), round(Recall,4), round(bal_acc,4), round(J,4), round(Specificity,4), round(Precision,4),\n",
    "                 round(NPV,4), round(F1,4), round(FBeta,4), round(MCC,4), round(Kappa,4),\n",
    "                 round(lower_bound,4), round(upper_bound,4)]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nComputed Evaluation Metrics (Binary):\")\n",
    "    print(mat_met.to_string(index=False))\n",
    "    \n",
    "    return mat_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e7063-15e6-45ab-a21e-35573b3a6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for training,  validation, and testing\n",
    "\n",
    "\"\"\"\n",
    "The input shape for a Keras model in TensorFlow is typically specified as (img_height, img_width, 3), where:\n",
    "img_height refers to the number of rows (height or vertical dimension).\n",
    "img_width refers to the number of columns (width or horizontal dimension).\n",
    "3 refers to the number of color channels (e.g., RGB).\n",
    "\"\"\"\n",
    "img_height, img_width = 168, 224\n",
    "\n",
    "train_data_dir = \"path/to/train\"\n",
    "val_data_dir = \"path/to/val\"\n",
    "test_data_dir = \"path/to/test\"\n",
    "\n",
    "epochs = 64\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "input_shape = (img_height, img_width, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803230e0-6ce5-4dfd-ad34-835bad67363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare data generators\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(\n",
    "        rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle = True,\n",
    "        class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        val_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size, \n",
    "        shuffle = False,\n",
    "        class_mode='categorical')\n",
    "\n",
    "#identify the number of samples\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_val_samples = len(val_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(val_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "Y_val=val_generator.classes\n",
    "print(Y_val.shape)\n",
    "Y_val1=to_categorical(Y_val,num_classes=num_classes)\n",
    "print(Y_val.shape)\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "Y_test1=to_categorical(Y_test,num_classes=num_classes)\n",
    "print(Y_test1.shape)\n",
    "train_steps = math.ceil(nb_train_samples / batch_size)\n",
    "val_steps = math.ceil(nb_val_samples / batch_size)\n",
    "test_steps = math.ceil(nb_test_samples / batch_size)\n",
    "print(train_steps)\n",
    "print(val_steps)\n",
    "print(test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9530e-ff45-48ee-9622-eb81d030c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare model architecture\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def create_model():\n",
    "    # Load the pre-trained model, excluding its top\n",
    "    model_base = VGG16(input_shape=(img_height, img_width, 3), include_top=False, weights='imagenet')\n",
    "    x = model_base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=model_base.input, outputs=predictions, name='normal_viral_model')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c074ca-a05b-4075-93ed-3a1b71afbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights (using scikit-learn)\n",
    "\n",
    "cw = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights = dict(enumerate(cw))\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41695328-8328-4515-8c9f-337fe01c5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare model checkpoint and store whenever validation loss decreases\n",
    "# In Keras 3, the recommended extension for saved models is now .keras.\n",
    "\n",
    "if not os.path.exists(\"weights\"):\n",
    "    os.makedirs(\"weights\")\n",
    "checkpoint_filepath = os.path.join(\"weights\", model.name + '.{epoch:02d}-{val_accuracy:.4f}.keras')\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    save_freq='epoch'\n",
    ")\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5,\n",
    "    verbose=1, mode='min', min_lr=0.00001\n",
    ")\n",
    "callbacks_list = [checkpoint, earlyStopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef93997-ca8f-4fe1-8d92-734382b1d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear session, reset generators, and begin training\n",
    "import time\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "train_generator.reset()\n",
    "val_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "epochs = epochs\n",
    "t0 = time.time()\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_steps=val_steps,\n",
    "    verbose=1,\n",
    "    class_weight=class_weights)\n",
    "print('Training time: {:.4f} seconds'.format(time.time() - t0))\n",
    "\n",
    "with open('train_history/normal-viral_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe5dd0-1ad0-464a-af24-067270aa6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance\n",
    "\n",
    "N = len(history) \n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=40)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "          history.history[\"accuracy\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         history.history[\"val_accuracy\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82555d-ecef-4a46-ab63-41e7b02439d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model for inference\n",
    "\n",
    "K.clear_session()\n",
    "model = load_model('weights/trained_model.keras',compile=False) #path to the trained model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4667dd4-9ded-49f3-aa5d-01c62f940b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate test predictions and display confusion matrix and other evaluation metrics values\n",
    "\n",
    "test_generator.reset()\n",
    "custom_y_test = model.predict(test_generator,\n",
    "                                    test_steps,\n",
    "                                    verbose=1)\n",
    "print(custom_y_test.shape)\n",
    "custom_y_test_label = custom_y_test.argmax(axis=-1)\n",
    "\n",
    "#compute metrics\n",
    "results_binary_test = comp_metrix_binary(Y_test, custom_y_test_label, beta=1, confidence=0.95, conf_vis=\"clinician\",\n",
    "                             annot_fontsize=14, tick_fontsize=12, title_fontsize=16, class_names=[\"Normal\", \"Viral\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf76b9-051c-4165-8d35-b69b816ce857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PR curve for the test data\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, \n",
    "                                 custom_y_test[:,1])\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "print(\"The area under the PR curve is\", metrics.auc(recall, precision))\n",
    "fig=plt.figure(figsize=(15,10), dpi=40)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_facecolor('white')\n",
    "major_ticks = np.arange(0.0, 1.1, 0.20) \n",
    "minor_ticks = np.arange(0.0, 1.1, 0.20)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "pyplot.plot(recall, precision, marker='.', color='green', label='Normal/Viral Model')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22875d58-1d5a-4192-8276-24dc926f0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the AUROC for the test data\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, custom_y_test[:,1])\n",
    "roc_auc = roc_auc_score(Y_test, custom_y_test[:,1])\n",
    "print(\"The area under the ROC curve is\", roc_auc)\n",
    "fig = plt.figure(figsize=(15,10), dpi=40)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_facecolor('white')\n",
    "major_ticks = np.arange(0.0, 1.1, 0.20) \n",
    "minor_ticks = np.arange(0.0, 1.1, 0.20)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "plt.plot(fpr, tpr, marker='.', color='green', label='Model')\n",
    "# axis labels\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6583b-545d-4b8d-b711-8bc29fbe80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP visualization\n",
    "\n",
    "import umap\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Clear any previous session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Load the trained model.\n",
    "model = load_model('weights/trained_model.keras',compile=False) #path to your final model\n",
    "\n",
    "# Function to extract embeddings from a given layer (e.g., block5_pool).\n",
    "def extract_embeddings(model, X_batch):\n",
    "    feature_layer_model = Model(inputs=model.inputs, outputs=model.get_layer('block5_pool').output)\n",
    "    return feature_layer_model.predict(X_batch)\n",
    "\n",
    "# Get feature embeddings and class labels from the test generator.\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(test_steps):\n",
    "    X_batch, y_batch = next(test_generator)\n",
    "    test_embeddings.extend(extract_embeddings(model, X_batch))\n",
    "    test_labels.extend(y_batch.argmax(axis=1))\n",
    "\n",
    "test_embeddings = np.array(test_embeddings).reshape(len(test_embeddings), -1)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Define UMAP hyperparameters to iterate over.\n",
    "neighbors = [2, 5, 10, 15, 20, 50, 100, 200]\n",
    "distance = [0.0, 0.1, 0.25, 0.5, 0.8, 0.99]\n",
    "custom_cmap = ListedColormap(['red', 'blue']) #for the two classes\n",
    "custom_class_names = ['Normal', 'Viral']\n",
    "\n",
    "# Tune UMAP hyperparameters and save the plots.\n",
    "for n in neighbors:\n",
    "    for d in distance:\n",
    "        umap_reducer = umap.UMAP(n_components=2, n_neighbors=n, min_dist=d, metric='euclidean', random_state=42)\n",
    "        umap_embeddings = umap_reducer.fit_transform(test_embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6), dpi=100)\n",
    "        plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1],\n",
    "                    c=test_labels, cmap=custom_cmap, alpha=0.9, s=2)\n",
    "        plt.title(f\"UMAP Visualization (n_neighbors: {n}, min_dist: {d})\")\n",
    "        \n",
    "        # Use the manually specified class names for legend.\n",
    "        handles = [plt.Line2D([], [], marker='o', linestyle='None', markersize=10,\n",
    "                               color=custom_cmap(i), label=custom_class_names[i])\n",
    "                   for i in range(len(custom_class_names))]\n",
    "        plt.legend(handles=handles, bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.)\n",
    "        \n",
    "        # Save the plot with 400 dpi resolution.\n",
    "        plt.savefig(f\"umap_normal_viral_n{n}_d{d}.png\", dpi=400, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9d7fc-2ef5-4553-be22-3b82414f6494",
   "metadata": {},
   "source": [
    "Entropy analysis: Compute the entropy associated with predicting each class (normal, bacterial, viral) using a trained model (one of normal/bacterial, normal/viral, bacterial/viral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b403f7-b847-4430-92bd-5bcf81eab4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session() \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Load model\n",
    "model = load_model('weights/trained_model.keras',compile=False) #path to the trained model\n",
    "\n",
    "# Set path to test directories\n",
    "test_dir_viral = \"path/to/test/viral\"\n",
    "test_dir_bacterial = \"path/to/test/bacterial\"\n",
    "test_dir_normal = \"path/to/test/normal\"\n",
    "\n",
    "img_height, img_width = 168, 224\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_and_preprocess_images(test_dir):\n",
    "    image_files = [f for f in os.listdir(test_dir) \n",
    "                   if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'))]\n",
    "    print(f\"Found {len(image_files)} images in {test_dir}.\")\n",
    "    \n",
    "    images = []\n",
    "    filenames = []\n",
    "    for img_name in image_files:\n",
    "        img_path = os.path.join(test_dir, img_name)\n",
    "        img = load_img(img_path, target_size=(img_height, img_width))\n",
    "        img_array = img_to_array(img) / 255.0\n",
    "        images.append(img_array)\n",
    "        filenames.append(img_name)\n",
    "        \n",
    "    images = np.array(images)\n",
    "    print(f\"Loaded images shape: {images.shape}\")\n",
    "    return images, filenames\n",
    "\n",
    "#custom function to compute entropy associated with the predictions\n",
    "def compute_entropy_for_test_set(test_dir):\n",
    "    images, filenames = load_and_preprocess_images(test_dir)\n",
    "    predictions = model.predict(images, verbose=1)  # shape: (n_samples, 2)\n",
    "    \n",
    "    def shannon_entropy(prob_vector):\n",
    "        eps = 1e-10  # avoid log(0)\n",
    "        p = np.clip(prob_vector, eps, 1.0)\n",
    "        return -np.sum(p * np.log2(p))\n",
    "    \n",
    "    entropies = np.array([shannon_entropy(prob) for prob in predictions])\n",
    "    df = pd.DataFrame({\n",
    "        'image': filenames,\n",
    "        'entropy': entropies\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Compute entropy for each test set\n",
    "\n",
    "df_viral = compute_entropy_for_test_set(test_dir_viral)\n",
    "df_viral['test_set'] = 'Viral'\n",
    "\n",
    "df_bacterial = compute_entropy_for_test_set(test_dir_bacterial)\n",
    "df_bacterial['test_set'] = 'Bacterial'\n",
    "\n",
    "df_normal = compute_entropy_for_test_set(test_dir_normal)\n",
    "df_normal['test_set'] = 'Normal'\n",
    "\n",
    "df_results = pd.concat([df_viral, df_bacterial, df_normal], axis=0).reset_index(drop=True)\n",
    "print(df_results.head())\n",
    "\n",
    "# Save to Excel\n",
    "excel_filename = 'entropy_three_class.xlsx'\n",
    "df_results.to_excel(excel_filename, index=False)\n",
    "print(f\"Results saved to {excel_filename}\")\n",
    "\n",
    "# Print statistics\n",
    "total_samples = len(df_results)\n",
    "for i, test_set_name in enumerate(unique_test_sets):\n",
    "    df_subset = df_results[df_results['test_set'] == test_set_name]\n",
    "    count = len(df_subset)\n",
    "    print(f\"\\nSummary Statistics for {test_set_name} Test Set:\")\n",
    "    print(f\"  Number of samples: {count} out of {total_samples} total samples\")\n",
    "    if count > 0:\n",
    "        print(f\"  Mean entropy: {df_subset['entropy'].mean():.4f}\")\n",
    "        print(f\"  Median entropy: {df_subset['entropy'].median():.4f}\")\n",
    "        print(f\"  Min entropy: {df_subset['entropy'].min():.4f}\")\n",
    "        print(f\"  Max entropy: {df_subset['entropy'].max():.4f}\")\n",
    "        print(f\"  1st Quartile (25%): {df_subset['entropy'].quantile(0.25):.4f}\")\n",
    "        print(f\"  3rd Quartile (75%): {df_subset['entropy'].quantile(0.75):.4f}\")\n",
    "    else:\n",
    "        print(\"  No samples for this test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb11c8-834a-4c20-b2b3-0269de710106",
   "metadata": {},
   "source": [
    "Computing statistical significance:\n",
    "\n",
    "Standard error for a â€œproportionâ€-style stat ğ‘ is approximated as ğ‘(1âˆ’ğ‘)/ğ‘›. Z-score tests how many SEs THE observed ğ‘ is from a null value (e.g.\\ 0.5 for quartiles, or 0 for IQR).\n",
    "In a classical Z-test we compute ğ‘=((observedÂ statistic)âˆ’(nullÂ value))/(standardÂ error), and then convert ğ‘ into a p-value. \n",
    "We treat Q1, median (Q2), and Q3 as â€œproportion-likeâ€ values on a scale of 0 to 1. \n",
    "By setting these values = 0.5, we are testing â€œIs the observed quartile significantly different from the midpoint of the scale (0.5)?â€\n",
    "In the context of entropy (which also ranges from 0 to logğ‘, but is often normalized), 0.5 is a convenient mid-point. \n",
    "If the data were perfectly uniform (maximum uncertainty), weâ€™d expect normalized entropy near 1; if very â€œcertain,â€ weâ€™d expect entropy near 0. Testing against 0.5 checks if the observed Q1/Q2/Q3 is meaningfully above or below that halfway mark.\n",
    "The IQR (inter-quartile range) is non-negative by definition. By setting null_iqr = 0.0, weâ€™re essentially testing â€œIs the spread between Q3 and Q1 statistically greater than zero?â€\n",
    "Any non-zero IQR indicates variability, but this formal Z-test asks whether that variability is significantly different from zero given your sample size.\n",
    "\n",
    "Interpretability: \n",
    "We assume 0.5 is the â€œcenterâ€ of a normalized 0â€“1 range. 0.0 is the natural lower bound for a range (IQR canâ€™t be negative).\n",
    "Hypothesis Framing\n",
    "For Q1/Q2/Q3:\n",
    "\n",
    "ğ»0: â€œQuartile = 0.5â€\n",
    "ğ»ğ´: â€œQuartile â‰  0.5â€\n",
    "\n",
    "For IQR:\n",
    "ğ»0: â€œIQR = 0â€ (no spread)\n",
    "ğ»ğ´: â€œIQR â‰  0â€\n",
    "\n",
    "Z-Test Formula: ğ‘stat=(observedâˆ’null)/SE\n",
    "Choosing a null provides the denominatorâ€™s reference point, so we compute a meaningful Z-score and corresponding p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ef060-a99a-464a-8952-b0e7a69c57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import norm\n",
    "\n",
    "def compute_wilson_ci(value, population, z=1.96):\n",
    "    \"\"\"\n",
    "    Computes the Wilson binomial confidence interval for a given statistic.\n",
    "    Here, 'value' is the statistic (e.g. quartile) computed from the sample,\n",
    "    and 'population' is the total number of samples.\n",
    "    \"\"\"\n",
    "    if population <= 0:\n",
    "        return float('nan'), float('nan')\n",
    "    p = value\n",
    "    n = population\n",
    "    sqrt_arg = (p * (1 - p) + (z**2) / (4 * n)) / n\n",
    "    lower_bound = (p + (z**2) / (2 * n) - z * math.sqrt(max(0, sqrt_arg))) / (1 + (z**2) / n)\n",
    "    upper_bound = (p + (z**2) / (2 * n) + z * math.sqrt(max(0, sqrt_arg))) / (1 + (z**2) / n)\n",
    "    return round(lower_bound, 4), round(upper_bound, 4)\n",
    "\n",
    "def get_contrasting_color(hex_color):\n",
    "    \"\"\"\n",
    "    Returns 'black' if the given hex color is light, else 'white'.\n",
    "    Uses Matplotlib's color conversion rather than seaborn.utils.hex2color.\n",
    "    \"\"\"\n",
    "    rgb = mcolors.to_rgb(hex_color)\n",
    "    luminance = 0.299 * rgb[0] + 0.587 * rgb[1] + 0.114 * rgb[2]\n",
    "    return \"black\" if luminance > 0.5 else \"white\"\n",
    "\n",
    "def plot_entropy_boxplot(df_results, \n",
    "                         target_class=\"Normal\", \n",
    "                         custom_labels=None,\n",
    "                         custom_colors=None,\n",
    "                         output_filename=\"entropy_boxplot.png\",\n",
    "                         annot_fontsize=14, \n",
    "                         tick_fontsize=12, \n",
    "                         title_fontsize=16):\n",
    "    \"\"\"\n",
    "    Create a boxplot of Shannon entropy values for three classes:\n",
    "    Normal, Bacterial, Viral. The 'df_results' DataFrame must have columns:\n",
    "      - 'test_set' with values in ['Normal', 'Bacterial', 'Viral']\n",
    "      - 'entropy' with numeric values.\n",
    "    Also computes Z-scores and two-tailed p-values for Q1, median, Q3, and IQR,\n",
    "    and prints significance flags (p<0.05) to the console.\n",
    "    \"\"\"\n",
    "    # Default labels/colors\n",
    "    if custom_labels is None:\n",
    "        custom_labels = {\"Normal\":\"Normal\",\"Bacterial\":\"Bacterial\",\"Viral\":\"Viral\"}\n",
    "    if custom_colors is None:\n",
    "        custom_colors = {\"Normal\":\"#3CB371\",\"Bacterial\":\"#4682B4\",\"Viral\":\"#FF6347\"}\n",
    "\n",
    "    # Validate target_class\n",
    "    all_classes = [\"Normal\",\"Bacterial\",\"Viral\"]\n",
    "    if target_class not in all_classes:\n",
    "        raise ValueError(f\"target_class must be one of {all_classes}\")\n",
    "\n",
    "    # Plot order so target_class appears first\n",
    "    if target_class==\"Viral\":\n",
    "        desired_order = [\"Viral\",\"Bacterial\",\"Normal\"]\n",
    "    elif target_class==\"Bacterial\":\n",
    "        desired_order = [\"Bacterial\",\"Viral\",\"Normal\"]\n",
    "    else:\n",
    "        desired_order = [\"Normal\",\"Bacterial\",\"Viral\"]\n",
    "\n",
    "    # Gather data, labels, colors\n",
    "    data, labels, box_colors, median_colors = [], [], [], []\n",
    "    for cls in desired_order:\n",
    "        vals = df_results.loc[df_results['test_set']==cls, 'entropy'].dropna().values\n",
    "        if vals.size > 0:\n",
    "            data.append(vals)\n",
    "            labels.append(custom_labels[cls])\n",
    "            c = custom_colors.get(cls, sns.color_palette(\"pastel\",1)[0])\n",
    "            box_colors.append(c)\n",
    "            median_colors.append(get_contrasting_color(c))\n",
    "        else:\n",
    "            print(f\"Warning: No entropy values found for class: {cls}\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data to plot for the selected classes.\")\n",
    "        return\n",
    "\n",
    "    # Create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(10,7), dpi=400, facecolor='white')\n",
    "    bp = ax.boxplot(data,\n",
    "                    patch_artist=True,\n",
    "                    notch=False,\n",
    "                    vert=True,\n",
    "                    showfliers=True,\n",
    "                    whis=1.5)\n",
    "\n",
    "    # Style boxes, whiskers, caps, medians, fliers\n",
    "    for patch, col in zip(bp['boxes'], box_colors):\n",
    "        patch.set_facecolor(col); patch.set_edgecolor(\"black\"); patch.set_linewidth(1.5)\n",
    "    for w in bp['whiskers']:\n",
    "        w.set(color='#8B008B', linewidth=1.5, linestyle=\":\")\n",
    "    for cap in bp['caps']:\n",
    "        cap.set(color='#8B008B', linewidth=2)\n",
    "    for median, mcol in zip(bp['medians'], median_colors):\n",
    "        median.set(color=mcol, linewidth=3)\n",
    "    for flier in bp['fliers']:\n",
    "        flier.set(marker='D', color='#e7298a', alpha=0.5)\n",
    "\n",
    "    ax.set_xticklabels(labels, fontsize=tick_fontsize)\n",
    "    ax.set_xlabel(\"Predicted Class\", fontsize=annot_fontsize)\n",
    "    ax.set_ylabel(\"Shannon Entropy\", fontsize=annot_fontsize)\n",
    "    ax.set_title(f\"Entropy Boxplot (Target: {custom_labels[target_class]})\",\n",
    "                 fontsize=title_fontsize)\n",
    "\n",
    "    # Annotate and compute stats\n",
    "    for i, d in enumerate(data):\n",
    "        n = len(d)\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        # Basic stats\n",
    "        min_v, max_v = d.min(), d.max()\n",
    "        q1 = np.percentile(d, 25)\n",
    "        med = np.median(d)\n",
    "        q3 = np.percentile(d, 75)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        # Wilson CIs\n",
    "        q1_lb, q1_ub = compute_wilson_ci(q1, n)\n",
    "        med_lb, med_ub = compute_wilson_ci(med, n)\n",
    "        q3_lb, q3_ub = compute_wilson_ci(q3, n)\n",
    "        iqr_lb, iqr_ub = compute_wilson_ci(iqr, n)\n",
    "\n",
    "        # 1) Compute standard errors (approximate for \"proportion-like\" stat)\n",
    "        se_q1 = math.sqrt(q1*(1 - q1)/n)\n",
    "        se_med = math.sqrt(med*(1 - med)/n)\n",
    "        se_q3 = math.sqrt(q3*(1 - q3)/n)\n",
    "        se_iqr = math.sqrt(se_q1**2 + se_q3**2)\n",
    "\n",
    "        # 2) Null hypotheses (test vs 0.5 for quartiles, vs 0 for IQR)\n",
    "        null_q1, null_med, null_q3, null_iqr = 0.5, 0.5, 0.5, 0.0\n",
    "\n",
    "        # 3) Z-scores\n",
    "        z_q1 = (q1  - null_q1)/se_q1\n",
    "        z_med = (med - null_med)/se_med\n",
    "        z_q3 = (q3  - null_q3)/se_q3\n",
    "        z_iqr = (iqr - null_iqr)/se_iqr\n",
    "\n",
    "        # 4) Two-tailed p-values\n",
    "        p_q1 = 2 * (1 - norm.cdf(abs(z_q1)))\n",
    "        p_med = 2 * (1 - norm.cdf(abs(z_med)))\n",
    "        p_q3 = 2 * (1 - norm.cdf(abs(z_q3)))\n",
    "        p_iqr = 2 * (1 - norm.cdf(abs(z_iqr)))\n",
    "\n",
    "        # 5) Significance flags\n",
    "        sig_q1 = \"statistically significant\" if p_q1 < 0.05 else \"not significant\"\n",
    "        sig_med = \"statistically significant\" if p_med < 0.05 else \"not significant\"\n",
    "        sig_q3 = \"statistically significant\" if p_q3 < 0.05 else \"not significant\"\n",
    "        sig_iqr = \"statistically significant\" if p_iqr < 0.05 else \"not significant\"\n",
    "\n",
    "        # 6) Console output\n",
    "        print(f\"\\nStatistics for {labels[i]} (n={n}):\")\n",
    "        print(f\" Q1 = {q1:.4f} [{q1_lb:.4f}, {q1_ub:.4f}], \"\n",
    "              f\"z = {z_q1:.2f}, p = {p_q1:.3f} â†’ {sig_q1}\")\n",
    "        print(f\" Median = {med:.4f} [{med_lb:.4f}, {med_ub:.4f}], \"\n",
    "              f\"z = {z_med:.2f}, p = {p_med:.3f} â†’ {sig_med}\")\n",
    "        print(f\" Q3 = {q3:.4f} [{q3_lb:.4f}, {q3_ub:.4f}], \"\n",
    "              f\"z = {z_q3:.2f}, p = {p_q3:.3f} â†’ {sig_q3}\")\n",
    "        print(f\" IQR = {iqr:.4f} [{iqr_lb:.4f}, {iqr_ub:.4f}], \"\n",
    "              f\"z = {z_iqr:.2f}, p = {p_iqr:.3f} â†’ {sig_iqr}\")\n",
    "\n",
    "        # Onâ€plot annotations (optional)\n",
    "        x_base = i + 1.05\n",
    "        ax.text(x_base, max_v, f\"Max: {max_v:.4f}\", va='bottom', fontsize=annot_fontsize-2)\n",
    "        ax.text(x_base, q1, f\"Q1 ({sig_q1})\", va='center', fontsize=annot_fontsize-2)\n",
    "        ax.text(x_base, med, f\"Med ({sig_med})\", va='center', fontsize=annot_fontsize-2)\n",
    "        ax.text(x_base, q3, f\"Q3 ({sig_q3})\", va='center', fontsize=annot_fontsize-2)\n",
    "        ax.text(x_base, q3 + 0.02*(ax.get_ylim()[1]-ax.get_ylim()[0]), f\"IQR ({sig_iqr})\", va='bottom', fontsize=annot_fontsize-2)\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    plt.savefig(output_filename, dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_entropy_boxplot(\n",
    "#     df_results,\n",
    "#     target_class=\"Viral\",\n",
    "#     custom_labels={\"Normal\":\"Normal\",\"Bacterial\":\"Bacterial\",\"Viral\":\"Viral\"},\n",
    "#     custom_colors={\"Normal\":\"#3CB371\",\"Bacterial\":\"#4682B4\",\"Viral\":\"#FF6347\"},\n",
    "#     output_filename=\"entropy_boxplot_with_ztest.png\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43ff8d-1a05-421e-ab6c-976ad2f968ef",
   "metadata": {},
   "source": [
    "END OF CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
